{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Question Classification \n",
    "### Implement Arabic questions classification using Support Vector Machine and Naive Bayes models. The data set consists of 1645 questions, splited into 2 classes (color and yes/no).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"Questions2.csv\" \n",
    "data = pd.read_csv(path_to_file, encoding = 'utf8') # load the dataset\n",
    "\n",
    "X = data.drop('Class', axis = 1) # removing the last column \n",
    "Y = data['Class'] # adding the last column\n",
    "\n",
    "# splitting the dataset to the training set and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, train_size = 0.8,random_state=2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec.load('full_grams_cbow_100_wiki.mdl') # load word embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function to get word vector for input question using \"full_grams_cbow_100_wiki\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(n_model,dim, token):\n",
    "    vec = np.zeros(dim) # initialization a list with zeros \n",
    "    is_vec = False\n",
    "    if token not in n_model.wv: \n",
    "        _count = 0\n",
    "        is_vec = True\n",
    "        for w in token.split(\" \"):\n",
    "            if w in n_model.wv:\n",
    "                _count += 1\n",
    "                vec += n_model.wv[w]\n",
    "        if _count > 0:\n",
    "            vec = vec / _count\n",
    "    else:\n",
    "        vec = n_model.wv[token]\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing steps:\n",
    "- Question to tokens\n",
    "- Tokens to vectors (using Arabic words embedding)\n",
    "- Merge the words vectors together\n",
    "- Padding the question vector to the maximum question length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train features\n",
    "question_train = [] #list to store training questions' vector \n",
    "for i in range(len(X_train)):\n",
    "    token = nltk.word_tokenize(X_train['Question'].values[i]) # questions to tokens \n",
    "    \n",
    "    token_vector = [] #list to store tokens vector \n",
    "    for i in range(len(token)):\n",
    "        token_vector.append(get_vec(word2vec_model,100,token[i])) # get word vector using pre-trained model\n",
    "    \n",
    "    token_vector = np.array(token_vector) # convert token vector list to array \n",
    "    token_vector = np.reshape(token_vector, (1, len(token_vector)*100)) # reshape array to 1 and token vector lenght\n",
    "    token_vector = np.pad(token_vector, ((0,0),(0,(1000 - token_vector.shape[1]))), 'constant') # paadding vector \n",
    "    question_train.append(token_vector) # add the token vector to the question list\n",
    "\n",
    "question_train = np.array(question_train) # convert question list to array \n",
    "question_train = np.reshape(question_train,(question_train.shape[0],question_train.shape[2])) # reshape the array to 2d\n",
    "  \n",
    "\n",
    "    \n",
    "# test features\n",
    "question_test = [] #list to store testing questions' vector \n",
    "for i in range(len(X_test)):\n",
    "    token = nltk.word_tokenize(X_test['Question'].values[i]) # questions to tokens \n",
    "   \n",
    "    token_vector_test = [] #list to store tokens vector \n",
    "    for i in range(len(token)):\n",
    "        token_vector_test.append(get_vec(word2vec_model,100,token[i])) # get word vector using pre-trained model\n",
    "        \n",
    "    token_vector_test = np.array(token_vector_test)# convert token vector list to array \n",
    "    token_vector_test = np.reshape(token_vector_test, (1, len(token_vector_test)*100))# reshape array to 1 and token vector lenght\n",
    "    token_vector_test = np.pad(token_vector_test, ((0,0),(0,(1000 - token_vector_test.shape[1]))), 'constant') # paadding vector \n",
    "    question_test.append(token_vector_test) # add the token vector to the question list\n",
    "    \n",
    "question_test = np.array(question_test) # convert question list to array\n",
    "question_test = np.reshape(question_test,(question_test.shape[0],question_test.shape[2])) # reshape the array to 2d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "### SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (SVM): 99.70\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma='auto')\n",
    "clf.fit(question_train, Y_train) # train the model \n",
    "print(\"Score (SVM): {:.2f}\".format(clf.score(question_test, Y_test)*100)) # calculate accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (Naive Bayes): 100.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf2 = GaussianNB()\n",
    "clf2.fit(question_train, Y_train) # train the model \n",
    "print(\"Score (Naive Bayes): {:.2f}\".format(clf2.score(question_test, Y_test)*100)) # calculate accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
